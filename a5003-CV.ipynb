{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 50 of 4000\n",
      "\n",
      "CASE 100 of 4000\n",
      "\n",
      "CASE 150 of 4000\n",
      "\n",
      "CASE 200 of 4000\n",
      "\n",
      "CASE 250 of 4000\n",
      "\n",
      "CASE 300 of 4000\n",
      "\n",
      "CASE 350 of 4000\n",
      "\n",
      "CASE 400 of 4000\n",
      "\n",
      "CASE 450 of 4000\n",
      "\n",
      "CASE 500 of 4000\n",
      "\n",
      "CASE 550 of 4000\n",
      "\n",
      "CASE 600 of 4000\n",
      "\n",
      "CASE 650 of 4000\n",
      "\n",
      "CASE 700 of 4000\n",
      "\n",
      "CASE 750 of 4000\n",
      "\n",
      "CASE 800 of 4000\n",
      "\n",
      "CASE 850 of 4000\n",
      "\n",
      "CASE 900 of 4000\n",
      "\n",
      "CASE 950 of 4000\n",
      "\n",
      "CASE 1000 of 4000\n",
      "\n",
      "CASE 1050 of 4000\n",
      "\n",
      "CASE 1100 of 4000\n",
      "\n",
      "CASE 1150 of 4000\n",
      "\n",
      "CASE 1200 of 4000\n",
      "\n",
      "CASE 1250 of 4000\n",
      "\n",
      "CASE 1300 of 4000\n",
      "\n",
      "CASE 1350 of 4000\n",
      "\n",
      "CASE 1400 of 4000\n",
      "\n",
      "CASE 1450 of 4000\n",
      "\n",
      "CASE 1500 of 4000\n",
      "\n",
      "CASE 1550 of 4000\n",
      "\n",
      "CASE 1600 of 4000\n",
      "\n",
      "CASE 1650 of 4000\n",
      "\n",
      "CASE 1700 of 4000\n",
      "\n",
      "CASE 1750 of 4000\n",
      "\n",
      "CASE 1800 of 4000\n",
      "\n",
      "CASE 1850 of 4000\n",
      "\n",
      "CASE 1900 of 4000\n",
      "\n",
      "CASE 1950 of 4000\n",
      "\n",
      "CASE 2000 of 4000\n",
      "\n",
      "CASE 2050 of 4000\n",
      "\n",
      "CASE 2100 of 4000\n",
      "\n",
      "CASE 2150 of 4000\n",
      "\n",
      "CASE 2200 of 4000\n",
      "\n",
      "CASE 2250 of 4000\n",
      "\n",
      "CASE 2300 of 4000\n",
      "\n",
      "CASE 2350 of 4000\n",
      "\n",
      "CASE 2400 of 4000\n",
      "\n",
      "CASE 2450 of 4000\n",
      "\n",
      "CASE 2500 of 4000\n",
      "\n",
      "CASE 2550 of 4000\n",
      "\n",
      "CASE 2600 of 4000\n",
      "\n",
      "CASE 2650 of 4000\n",
      "\n",
      "CASE 2700 of 4000\n",
      "\n",
      "CASE 2750 of 4000\n",
      "\n",
      "CASE 2800 of 4000\n",
      "\n",
      "CASE 2850 of 4000\n",
      "\n",
      "CASE 2900 of 4000\n",
      "\n",
      "CASE 2950 of 4000\n",
      "\n",
      "CASE 3000 of 4000\n",
      "\n",
      "CASE 3050 of 4000\n",
      "\n",
      "CASE 3100 of 4000\n",
      "\n",
      "CASE 3150 of 4000\n",
      "\n",
      "CASE 3200 of 4000\n",
      "\n",
      "CASE 3250 of 4000\n",
      "\n",
      "CASE 3300 of 4000\n",
      "\n",
      "CASE 3350 of 4000\n",
      "\n",
      "CASE 3400 of 4000\n",
      "\n",
      "CASE 3450 of 4000\n",
      "\n",
      "CASE 3500 of 4000\n",
      "\n",
      "CASE 3550 of 4000\n",
      "\n",
      "CASE 3600 of 4000\n",
      "\n",
      "CASE 3650 of 4000\n",
      "\n",
      "CASE 3700 of 4000\n",
      "\n",
      "CASE 3750 of 4000\n",
      "\n",
      "CASE 3800 of 4000\n",
      "\n",
      "CASE 3850 of 4000\n",
      "\n",
      "CASE 3900 of 4000\n",
      "\n",
      "CASE 3950 of 4000\n",
      "\n",
      "CASE 4000 of 4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd       \n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Open Trining File\n",
    "os.chdir(\"/temp/DocumentCorpus/ECHR/echr_dataset-master/echr_dataset-master/document_crawler/IN/\")\n",
    "train = pd.read_csv(\"TRAIN_classifiedFULL_NOADMISS.tsv\", header=0, delimiter=\"^\", quoting=3)\n",
    "test  = pd.read_csv(\"TEST_NOclassifiedFULL_NOADMISS.tsv\", header=0, delimiter=\"^\", quoting=3)\n",
    "\n",
    "#os.chdir(\"/temp/DocumentCorpus/ECHR/echr_dataset-master/echr_dataset-master/document_crawler/OUT/AV3/\")\n",
    "os.chdir(\"/temp/DocumentCorpus/ECHR/echr_dataset-master/echr_dataset-master/document_crawler/OUT/AV5/CV/\")\n",
    "\n",
    "train.shape\n",
    "test.shape\n",
    "\n",
    "train.columns.values\n",
    "\n",
    "from bs4 import BeautifulSoup   \n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 5a. Remove extra Stop words\n",
    "    moreStopwords = [\"b\",\"full\",\"value\",\"file\",\"period\",\"months\",\"additional\",\"basis\",\"street\",\"respect\",\"cases\",\"application\",\"amount\",\"since\",\"costs\",\"address\",\"well\",\"days\",\"series\",\"c\",\"particular\",\"purposes\",\"purpose\",\"proceedings\",\"proceeding\",\"h\",\"able\",\"ba\",\"country\",\"held\",\"board\",\"first\",\"second\",\"final\",\"judge\",\"non\",\"statement\",\"documents\",\"many\",\"notes\",\"note\",\"j\",\"considered\",\"aw\",\"echr\",\"whether\",\"language\",\"ill\",\"time\",\"taken\",\"kh\",\"rovd\",\"must\",\"set\",\"within\",\"p\",\"mr\",\"mrs\",\"provided\",\"sher\",\"one\",\"new\",\"route\",\"routes\",\"three\",\"would\",\"previously\",\"shall\",\"en\",\"k\",\"g\",\"applicants\",\"eur\",\"date\",\"might\",\"paragraph\",\"u\",\"kd\",\"could\",\"made\",\"company\",\"see\",\"public\",\"parking\",\"statements\",\"article\",\"government\",\"business\",\"information\",\"therefore\",\"right\",\"also\",\"applicant\",\"court\",\"act\",\"state\",\"security\",\"section\",\"hearing\",\"v\",\"service\",\"case\",\"law\",\"person\",\"courts\",\"regional\",\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "    more_meaningful_words = [token for token in meaningful_words if token not in moreStopwords]\n",
    "    \n",
    "    #\n",
    "    # 6. Stemmer\n",
    "    #stemmed=[]\n",
    "    #stemmer = PorterStemmer()\n",
    "    #for word in more_meaningful_words:\n",
    "    #    stemmed.append(stemmer.stem(word))\n",
    "        \n",
    "    #\n",
    "    # 7. Lemmatizer\n",
    "    Lemmatized=[]\n",
    "    Lemmatizer = WordNetLemmatizer()\n",
    "    #for word in meaningful_words:\n",
    "    for word in more_meaningful_words:\n",
    "        \n",
    "        Lemmatized.append(Lemmatizer.lemmatize(word))\n",
    "        \n",
    "    #\n",
    "    # 8. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( Lemmatized ))  \n",
    "    #return( \" \".join( stemmed ))  \n",
    "    #return( \" \".join( more_meaningful_words ))  \n",
    "    #return( \" \".join( meaningful_words ))   \n",
    "    \n",
    "# Get the number of cases based on the dataframe column size\n",
    "\n",
    "num_reviews = train[\"sentences\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean cases\n",
    "clean_train_sentences = []\n",
    "\n",
    "# Loop over each case \n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 100, print a message\n",
    "    if( (i+1)%50 == 0 ):\n",
    "        print \"CASE %d of %d\\n\" % ( i+1, num_reviews )                                                                    \n",
    "    clean_train_sentences.append( review_to_words( train[\"sentences\"][i] ))    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000L, 5000L)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             ngram_range=(2, 2),  \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features=[]\n",
    "train_data_features = vectorizer.fit_transform(clean_train_sentences)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "print(train_data_features.shape)\n",
    "\n",
    "#vocab = vectorizer.get_feature_names()\n",
    "#print(vocab)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\danny\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For each, print the vocabulary word and the number of times it \n",
    "## appears in the training set\n",
    "#for tag, count in zip(vocab, dist):\n",
    "#    print count, tag\n",
    "    \n",
    "#print \"Training the random forest...\"\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#\n",
    "## Initialize a Random Forest classifier with 100 trees\n",
    "#forest = RandomForestClassifier(n_estimators = 50) \n",
    "#\n",
    "## Fit the forest to the training set, using the bag of words as \n",
    "## features and the sentiment labels as the response variable\n",
    "##\n",
    "## This may take a few minutes to run\n",
    "#forest = forest.fit( train_data_features, train[\"AV3\"] )    \n",
    "\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "grid_values = {} #{'C':[30]} # Decide which settings you want for the grid search. \n",
    "model_LR = GridSearchCV(RandomForestClassifier(n_estimators = 100), \n",
    "                        grid_values, \n",
    "                        scoring = 'roc_auc', cv = 20) \n",
    "\n",
    "# Try to set the scoring on what the contest is asking for. \n",
    "# The contest says scoring is for area under the ROC curve, so use this.\n",
    "                            \n",
    "model_LR.fit(train_data_features, train[\"AV5\"] ) # Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.98044, std: 0.01238, params: {}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2348, 9)\n",
      "Cleaning and parsing the test set ECHR Cases...\n",
      "\n",
      "Case 100 of 2348\n",
      "\n",
      "Case 200 of 2348\n",
      "\n",
      "Case 300 of 2348\n",
      "\n",
      "Case 400 of 2348\n",
      "\n",
      "Case 500 of 2348\n",
      "\n",
      "Case 600 of 2348\n",
      "\n",
      "Case 700 of 2348\n",
      "\n",
      "Case 800 of 2348\n",
      "\n",
      "Case 900 of 2348\n",
      "\n",
      "Case 1000 of 2348\n",
      "\n",
      "Case 1100 of 2348\n",
      "\n",
      "Case 1200 of 2348\n",
      "\n",
      "Case 1300 of 2348\n",
      "\n",
      "Case 1400 of 2348\n",
      "\n",
      "Case 1500 of 2348\n",
      "\n",
      "Case 1600 of 2348\n",
      "\n",
      "Case 1700 of 2348\n",
      "\n",
      "Case 1800 of 2348\n",
      "\n",
      "Case 1900 of 2348\n",
      "\n",
      "Case 2000 of 2348\n",
      "\n",
      "Case 2100 of 2348\n",
      "\n",
      "Case 2200 of 2348\n",
      "\n",
      "Case 2300 of 2348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print test.shape\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test[\"sentences\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "print \"Cleaning and parsing the test set ECHR Cases...\\n\"\n",
    "for i in xrange(0,num_reviews):\n",
    "    if( (i+1) % 100 == 0 ):\n",
    "        print \"Case %d of %d\\n\" % (i+1, num_reviews)\n",
    "    clean_review = review_to_words( test[\"sentences\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "## Use the random forest to make sentiment label predictions\n",
    "#result = forest.predict(test_data_features)\n",
    "#\n",
    "## Copy the results to a pandas dataframe with an \"id\" column and\n",
    "## a \"sentiment\" column\n",
    "#output = pd.DataFrame( data={\"title\":test[\"title\"],\"conclusion\":test[\"conclusion\"], \"AV3_Predict\":result} )\n",
    "#\n",
    "## Use pandas to write the comma-separated output file\n",
    "#output.to_csv( \"Bag_of_Words_LEMM_100RF_CV.csv\", index=False, quoting=3, escapechar='\\\\', sep='^' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LR_result = model_LR.predict_proba(test_data_features)[:,1] \n",
    "#LR_output = pd.DataFrame(data={\"id\":test[\"id\"], \"AV3\":LR_result}) # Create our dataframe that will be written.\n",
    "LR_output = pd.DataFrame( data={\"title\":test[\"title\"],\"conclusion\":test[\"conclusion\"], \"AV5_Predict\":LR_result} )\n",
    "#LR_output.to_csv('Logistic_Reg_Proj2.csv', index=False, quoting=3) # Get the .csv file we will submit to Kaggle.\n",
    "LR_output.to_csv( \"A5003_Bag_of_Words_LEMM_100RF_BI_CV.csv\", index=False, quoting=3, escapechar='\\\\', sep='^'  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
