{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 50 of 4000\n",
      "\n",
      "CASE 100 of 4000\n",
      "\n",
      "CASE 150 of 4000\n",
      "\n",
      "CASE 200 of 4000\n",
      "\n",
      "CASE 250 of 4000\n",
      "\n",
      "CASE 300 of 4000\n",
      "\n",
      "CASE 350 of 4000\n",
      "\n",
      "CASE 400 of 4000\n",
      "\n",
      "CASE 450 of 4000\n",
      "\n",
      "CASE 500 of 4000\n",
      "\n",
      "CASE 550 of 4000\n",
      "\n",
      "CASE 600 of 4000\n",
      "\n",
      "CASE 650 of 4000\n",
      "\n",
      "CASE 700 of 4000\n",
      "\n",
      "CASE 750 of 4000\n",
      "\n",
      "CASE 800 of 4000\n",
      "\n",
      "CASE 850 of 4000\n",
      "\n",
      "CASE 900 of 4000\n",
      "\n",
      "CASE 950 of 4000\n",
      "\n",
      "CASE 1000 of 4000\n",
      "\n",
      "CASE 1050 of 4000\n",
      "\n",
      "CASE 1100 of 4000\n",
      "\n",
      "CASE 1150 of 4000\n",
      "\n",
      "CASE 1200 of 4000\n",
      "\n",
      "CASE 1250 of 4000\n",
      "\n",
      "CASE 1300 of 4000\n",
      "\n",
      "CASE 1350 of 4000\n",
      "\n",
      "CASE 1400 of 4000\n",
      "\n",
      "CASE 1450 of 4000\n",
      "\n",
      "CASE 1500 of 4000\n",
      "\n",
      "CASE 1550 of 4000\n",
      "\n",
      "CASE 1600 of 4000\n",
      "\n",
      "CASE 1650 of 4000\n",
      "\n",
      "CASE 1700 of 4000\n",
      "\n",
      "CASE 1750 of 4000\n",
      "\n",
      "CASE 1800 of 4000\n",
      "\n",
      "CASE 1850 of 4000\n",
      "\n",
      "CASE 1900 of 4000\n",
      "\n",
      "CASE 1950 of 4000\n",
      "\n",
      "CASE 2000 of 4000\n",
      "\n",
      "CASE 2050 of 4000\n",
      "\n",
      "CASE 2100 of 4000\n",
      "\n",
      "CASE 2150 of 4000\n",
      "\n",
      "CASE 2200 of 4000\n",
      "\n",
      "CASE 2250 of 4000\n",
      "\n",
      "CASE 2300 of 4000\n",
      "\n",
      "CASE 2350 of 4000\n",
      "\n",
      "CASE 2400 of 4000\n",
      "\n",
      "CASE 2450 of 4000\n",
      "\n",
      "CASE 2500 of 4000\n",
      "\n",
      "CASE 2550 of 4000\n",
      "\n",
      "CASE 2600 of 4000\n",
      "\n",
      "CASE 2650 of 4000\n",
      "\n",
      "CASE 2700 of 4000\n",
      "\n",
      "CASE 2750 of 4000\n",
      "\n",
      "CASE 2800 of 4000\n",
      "\n",
      "CASE 2850 of 4000\n",
      "\n",
      "CASE 2900 of 4000\n",
      "\n",
      "CASE 2950 of 4000\n",
      "\n",
      "CASE 3000 of 4000\n",
      "\n",
      "CASE 3050 of 4000\n",
      "\n",
      "CASE 3100 of 4000\n",
      "\n",
      "CASE 3150 of 4000\n",
      "\n",
      "CASE 3200 of 4000\n",
      "\n",
      "CASE 3250 of 4000\n",
      "\n",
      "CASE 3300 of 4000\n",
      "\n",
      "CASE 3350 of 4000\n",
      "\n",
      "CASE 3400 of 4000\n",
      "\n",
      "CASE 3450 of 4000\n",
      "\n",
      "CASE 3500 of 4000\n",
      "\n",
      "CASE 3550 of 4000\n",
      "\n",
      "CASE 3600 of 4000\n",
      "\n",
      "CASE 3650 of 4000\n",
      "\n",
      "CASE 3700 of 4000\n",
      "\n",
      "CASE 3750 of 4000\n",
      "\n",
      "CASE 3800 of 4000\n",
      "\n",
      "CASE 3850 of 4000\n",
      "\n",
      "CASE 3900 of 4000\n",
      "\n",
      "CASE 3950 of 4000\n",
      "\n",
      "CASE 4000 of 4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd       \n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Open Trining File\n",
    "os.chdir(\"/temp/DocumentCorpus/ECHR/echr_dataset-master/echr_dataset-master/document_crawler/IN/\")\n",
    "train = pd.read_csv(\"TRAIN_classifiedFULL_NOADMISS.tsv\", header=0, delimiter=\"^\", quoting=3)\n",
    "test  = pd.read_csv(\"TEST_NOclassifiedFULL_NOADMISS.tsv\", header=0, delimiter=\"^\", quoting=3)\n",
    "\n",
    "os.chdir(\"/temp/DocumentCorpus/ECHR/echr_dataset-master/echr_dataset-master/document_crawler/OUT/AV3/CV/\")\n",
    "\n",
    "train.shape\n",
    "test.shape\n",
    "\n",
    "train.columns.values\n",
    "\n",
    "from bs4 import BeautifulSoup   \n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 5a. Remove extra Stop words\n",
    "    moreStopwords = [\"b\",\"full\",\"value\",\"file\",\"period\",\"months\",\"additional\",\"basis\",\"street\",\"respect\",\"cases\",\"application\",\"amount\",\"since\",\"costs\",\"address\",\"well\",\"days\",\"series\",\"c\",\"particular\",\"purposes\",\"purpose\",\"proceedings\",\"proceeding\",\"h\",\"able\",\"ba\",\"country\",\"held\",\"board\",\"first\",\"second\",\"final\",\"judge\",\"non\",\"statement\",\"documents\",\"many\",\"notes\",\"note\",\"j\",\"considered\",\"aw\",\"echr\",\"whether\",\"language\",\"ill\",\"time\",\"taken\",\"kh\",\"rovd\",\"must\",\"set\",\"within\",\"p\",\"mr\",\"mrs\",\"provided\",\"sher\",\"one\",\"new\",\"route\",\"routes\",\"three\",\"would\",\"previously\",\"shall\",\"en\",\"k\",\"g\",\"applicants\",\"eur\",\"date\",\"might\",\"paragraph\",\"u\",\"kd\",\"could\",\"made\",\"company\",\"see\",\"public\",\"parking\",\"statements\",\"article\",\"government\",\"business\",\"information\",\"therefore\",\"right\",\"also\",\"applicant\",\"court\",\"act\",\"state\",\"security\",\"section\",\"hearing\",\"v\",\"service\",\"case\",\"law\",\"person\",\"courts\",\"regional\",\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "    more_meaningful_words = [token for token in meaningful_words if token not in moreStopwords]\n",
    "    \n",
    "    #\n",
    "    # 6. Stemmer\n",
    "    #stemmed=[]\n",
    "    #stemmer = PorterStemmer()\n",
    "    #for word in more_meaningful_words:\n",
    "    #    stemmed.append(stemmer.stem(word))\n",
    "        \n",
    "    #\n",
    "    # 7. Lemmatizer\n",
    "    Lemmatized=[]\n",
    "    Lemmatizer = WordNetLemmatizer()\n",
    "    for word in more_meaningful_words:\n",
    "        Lemmatized.append(Lemmatizer.lemmatize(word))\n",
    "        \n",
    "    #\n",
    "    # 8. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( Lemmatized ))  \n",
    "    #return( \" \".join( stemmed ))  \n",
    "    #return( \" \".join( more_meaningful_words ))  \n",
    "    #return( \" \".join( meaningful_words ))   \n",
    "    \n",
    "# Get the number of cases based on the dataframe column size\n",
    "\n",
    "num_reviews = train[\"sentences\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean cases\n",
    "clean_train_sentences = []\n",
    "\n",
    "# Loop over each case \n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 100, print a message\n",
    "    if( (i+1)%50 == 0 ):\n",
    "        print \"CASE %d of %d\\n\" % ( i+1, num_reviews )                                                                    \n",
    "    clean_train_sentences.append( review_to_words( train[\"sentences\"][i] ))    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000L, 81L)\n",
      "[u'admissibility fact', u'admissibility merit', u'appeal decision', u'article convention', u'become circumstance', u'become convention', u'born life', u'circumstance born', u'circumstance convention', u'civil right', u'code criminal', u'complaint concerning', u'contested argument', u'convention complained', u'convention decided', u'convention read', u'convention subject', u'criminal code', u'criminal offence', u'criminal procedure', u'decided examine', u'decided give', u'decided rule', u'degrading treatment', u'deputy registrar', u'determination civil', u'dismissed appeal', u'domestic practice', u'entitled fair', u'everyone entitled', u'examine merit', u'far relevant', u'federation european', u'federation lodged', u'fifth sitting', u'follows determination', u'follows everyone', u'foreign affair', u'fourth sitting', u'give notice', u'inhuman degrading', u'inter alia', u'judge ren', u'khanlar hajiyev', u'lawyer practising', u'merit admissibility', u'merit fact', u'ministry foreign', u'ministry justice', u'nielsen registrar', u'notice decided', u'obligation everyone', u'originated republic', u'originated russian', u'police officer', u'pre trial', u'prosecutor office', u'provision convention', u'read follows', u'relevant part', u'relevant provision', u'relied convention', u'ren nielsen', u'representative russian', u'represented agent', u'right fifth', u'right fourth', u'right freedom', u'right obligation', u'right sitting', u'rule admissibility', u'rule rule', u'russia european', u'russia judgment', u'russian federation', u'russian national', u'russian represented', u'subjected torture', u'torture inhuman', u'treatment punishment', u'year imprisonment']\n",
      "178.917313155 admissibility fact\n",
      "216.191343577 admissibility merit\n",
      "199.752618084 appeal decision\n",
      "191.362022692 article convention\n",
      "157.683660846 become circumstance\n",
      "218.463021122 become convention\n",
      "258.802105033 born life\n",
      "254.951499517 circumstance born\n",
      "157.915645915 circumstance convention\n",
      "233.18726477 civil right\n",
      "253.308055678 code criminal\n",
      "157.421650844 complaint concerning\n",
      "177.942119185 contested argument\n",
      "262.812617156 convention complained\n",
      "167.675256632 convention decided\n",
      "231.608438996 convention read\n",
      "249.387945555 convention subject\n",
      "272.330534019 criminal code\n",
      "214.272809967 criminal offence\n",
      "281.175750267 criminal procedure\n",
      "197.961460739 decided examine\n",
      "220.788101288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\danny\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " decided give\n",
      "191.713510056 decided rule\n",
      "175.098446152 degrading treatment\n",
      "143.139395945 deputy registrar\n",
      "226.696385881 determination civil\n",
      "211.570957692 dismissed appeal\n",
      "259.081041628 domestic practice\n",
      "171.546404466 entitled fair\n",
      "243.599784722 everyone entitled\n",
      "197.634063916 examine merit\n",
      "199.50821908 far relevant\n",
      "107.564973434 federation european\n",
      "106.088178865 federation lodged\n",
      "152.078683921 fifth sitting\n",
      "227.117095008 follows determination\n",
      "158.052000165 follows everyone\n",
      "156.088985498 foreign affair\n",
      "161.226068127 fourth sitting\n",
      "220.059416018 give notice\n",
      "180.054065208 inhuman degrading\n",
      "356.329371206 inter alia\n",
      "104.119157184 judge ren\n",
      "103.386910253 khanlar hajiyev\n",
      "249.263961097 lawyer practising\n",
      "185.734770395 merit admissibility\n",
      "166.779025976 merit fact\n",
      "149.55774436 ministry foreign\n",
      "183.778792176 ministry justice\n",
      "117.367913968 nielsen registrar\n",
      "169.651494968 notice decided\n",
      "219.080079506 obligation everyone\n",
      "223.660980828 originated republic\n",
      "100.787565469 originated russian\n",
      "434.118481165 police officer\n",
      "326.933597689 pre trial\n",
      "454.902509479 prosecutor office\n",
      "133.777448228 provision convention\n",
      "410.509885752 read follows\n",
      "228.576377154 relevant part\n",
      "212.650259609 relevant provision\n",
      "127.242930819 relied convention\n",
      "105.26258974 ren nielsen\n",
      "110.021002767 representative russian\n",
      "267.142633292 represented agent\n",
      "150.716429135 right fifth\n",
      "161.257702962 right fourth\n",
      "192.665268502 right freedom\n",
      "251.211865156 right obligation\n",
      "190.922534196 right sitting\n",
      "192.411030115 rule admissibility\n",
      "200.30262374 rule rule\n",
      "105.345546382 russia european\n",
      "102.057397372 russia judgment\n",
      "372.469348032 russian federation\n",
      "111.866310836 russian national\n",
      "98.8969593518 russian represented\n",
      "113.446243441 subjected torture\n",
      "131.265800728 torture inhuman\n",
      "140.756943344 treatment punishment\n",
      "191.148694918 year imprisonment\n",
      "Training the random forest...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.96683, std: 0.02593, params: {}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "#vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "#                             ngram_range=(1, 2),  \\\n",
    "#                             tokenizer = None,    \\\n",
    "#                             preprocessor = None, \\\n",
    "#                             stop_words = None,   \\\n",
    "#                             max_features = 5000) \n",
    "\n",
    "# Initialize the \"TfidfVectorizer\" object, which is scikitlearn's tf/idf tool.\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, \\\n",
    "                             max_features=5000, \\\n",
    "                             min_df=0.2, \\\n",
    "                             stop_words=None, \\\n",
    "                             use_idf=True, \\\n",
    "                             tokenizer=None, \\\n",
    "                             ngram_range=(2,2))\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features=[]\n",
    "train_data_features = vectorizer.fit_transform(clean_train_sentences)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "print(train_data_features.shape)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag\n",
    "    \n",
    "print \"Training the random forest...\"\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#\n",
    "## Initialize a Random Forest classifier with 100 trees\n",
    "#forest = RandomForestClassifier(n_estimators = 100) \n",
    "#\n",
    "## Fit the forest to the training set, using the bag of words as \n",
    "## features and the sentiment labels as the response variable\n",
    "##\n",
    "## This may take a few minutes to run\n",
    "#forest = forest.fit( train_data_features, train[\"AV3\"] )    \n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "grid_values = {} #{'C':[30]} # Decide which settings you want for the grid search. \n",
    "\n",
    "model_LR = GridSearchCV(RandomForestClassifier(n_estimators = 100), \n",
    "                        grid_values, \n",
    "                        scoring = 'roc_auc', cv = 20) \n",
    "\n",
    "# Try to set the scoring on what the contest is asking for. \n",
    "# The contest says scoring is for area under the ROC curve, so use this.\n",
    "                            \n",
    "model_LR.fit(train_data_features, train[\"AV3\"] ) # Fit the model.\n",
    "\n",
    "model_LR.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2348, 9)\n",
      "Cleaning and parsing the test set ECHR Cases...\n",
      "\n",
      "Case 100 of 2348\n",
      "\n",
      "Case 200 of 2348\n",
      "\n",
      "Case 300 of 2348\n",
      "\n",
      "Case 400 of 2348\n",
      "\n",
      "Case 500 of 2348\n",
      "\n",
      "Case 600 of 2348\n",
      "\n",
      "Case 700 of 2348\n",
      "\n",
      "Case 800 of 2348\n",
      "\n",
      "Case 900 of 2348\n",
      "\n",
      "Case 1000 of 2348\n",
      "\n",
      "Case 1100 of 2348\n",
      "\n",
      "Case 1200 of 2348\n",
      "\n",
      "Case 1300 of 2348\n",
      "\n",
      "Case 1400 of 2348\n",
      "\n",
      "Case 1500 of 2348\n",
      "\n",
      "Case 1600 of 2348\n",
      "\n",
      "Case 1700 of 2348\n",
      "\n",
      "Case 1800 of 2348\n",
      "\n",
      "Case 1900 of 2348\n",
      "\n",
      "Case 2000 of 2348\n",
      "\n",
      "Case 2100 of 2348\n",
      "\n",
      "Case 2200 of 2348\n",
      "\n",
      "Case 2300 of 2348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print test.shape\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test[\"sentences\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "print \"Cleaning and parsing the test set ECHR Cases...\\n\"\n",
    "for i in xrange(0,num_reviews):\n",
    "    if( (i+1) % 100 == 0 ):\n",
    "        print \"Case %d of %d\\n\" % (i+1, num_reviews)\n",
    "    clean_review = review_to_words( test[\"sentences\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "## Use the random forest to make sentiment label predictions\n",
    "#result = forest.predict(test_data_features)\n",
    "#\n",
    "## Copy the results to a pandas dataframe with an \"id\" column and\n",
    "## a \"sentiment\" column\n",
    "#output = pd.DataFrame( data={\"title\":test[\"title\"],\"conclusion\":test[\"conclusion\"], \"AV3_Predict\":result} )\n",
    "#\n",
    "## Use pandas to write the comma-separated output file\n",
    "#output.to_csv( \"TFIDF_LEMM_BI.csv\", index=False, quoting=3, escapechar='\\\\', sep='^' )\n",
    "\n",
    "LR_result = model_LR.predict_proba(test_data_features)[:,1] \n",
    "#LR_output = pd.DataFrame(data={\"id\":test[\"id\"], \"AV3\":LR_result}) # Create our dataframe that will be written.\n",
    "LR_output = pd.DataFrame( data={\"title\":test[\"title\"],\"conclusion\":test[\"conclusion\"], \"AV3_Predict\":LR_result} )\n",
    "#LR_output.to_csv('Logistic_Reg_Proj2.csv', index=False, quoting=3) # Get the .csv file we will submit to Kaggle.\n",
    "LR_output.to_csv( \"A3005_TFIDF_LEMM_BI_CV.csv\", index=False, quoting=3, escapechar='\\\\', sep='^'  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
